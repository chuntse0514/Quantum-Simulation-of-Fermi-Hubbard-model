nohup: ignoring input
spin up orbital energies: {0: -3.0, 2: -1.0, 4: 0, 6: 2.0, 8: 0, 10: 2.0}
spin down orbital energies:  {1: -3.0, 3: -1.0, 5: 0, 7: 2.0, 9: 0, 11: 2.0}
spin up indices:  [0, 2, 4, 8]    spin down indices:  [1, 3] 

ground state energy:  -5.590291293562759
particle number:  6

learning rate =  0.03333332929378318
Find operators
-1j [3^ 2^ 10 7] + 1j [10^ 7^ 3 2]
-1j [4^ 1^ 11 10] + 1j [11^ 10^ 4 1]
1j [2^ 1^ 10 5] + -1j [10^ 5^ 2 1]
1j [1^ 0^ 11 6] + -1j [11^ 6^ 1 0]
1j [4^ 3^ 10 9] + -1j [10^ 9^ 4 3]
-1j [1^ 0^ 10 7] + 1j [10^ 7^ 1 0]
1j [3^ 2^ 11 6] + -1j [11^ 6^ 3 2]
1j [3^ 0^ 9 6] + -1j [9^ 6^ 3 0]
-1j [2^ 1^ 9 6] + 1j [9^ 6^ 2 1]
-1j [7^ 6^ 8 1] + 1j [8^ 1^ 7 6]
-1j [3^ 0^ 10 5] + 1j [10^ 5^ 3 0]
1j [6^ 5^ 8 3] + -1j [8^ 3^ 6 5]
with max gradients
[0.6666666, 0.6666666, 0.6666666, 0.6666666, 0.6666666, 0.6666666, 0.6666666, 0.6666666, 0.6666666, 0.6666666, 0.6666666, 0.6666666]

iter: 1 | loss: -5.333333 | norm:  2.309401 | fidelity:  0.941224 | Sz:  1.000000 | S^2:  2.000000
iter: 2 | loss: -5.516897 | norm:  0.999197 | fidelity:  0.977370 | Sz:  0.999996 | S^2:  1.999994
iter: 3 | loss: -5.548253 | norm:  0.855716 | fidelity:  0.989574 | Sz:  1.000000 | S^2:  2.000037
iter: 4 | loss: -5.523855 | norm:  1.236088 | fidelity:  0.989758 | Sz:  1.000001 | S^2:  2.000127
iter: 5 | loss: -5.517027 | norm:  1.262229 | fidelity:  0.989537 | Sz:  0.999999 | S^2:  2.000218
iter: 6 | loss: -5.530211 | norm:  1.053578 | fidelity:  0.990871 | Sz:  0.999998 | S^2:  2.000269
iter: 7 | loss: -5.544681 | norm:  0.815389 | fidelity:  0.992148 | Sz:  0.999999 | S^2:  2.000270
iter: 8 | loss: -5.550967 | norm:  0.702671 | fidelity:  0.992500 | Sz:  1.000002 | S^2:  2.000257
iter: 9 | loss: -5.549099 | norm:  0.740958 | fidelity:  0.991992 | Sz:  1.000000 | S^2:  2.000259
iter: 10 | loss: -5.544153 | norm:  0.833886 | fidelity:  0.991341 | Sz:  1.000001 | S^2:  2.000302
iter: 11 | loss: -5.543008 | norm:  0.879086 | fidelity:  0.991506 | Sz:  1.000000 | S^2:  2.000356
iter: 12 | loss: -5.548947 | norm:  0.834309 | fidelity:  0.992827 | Sz:  1.000000 | S^2:  2.000387
iter: 13 | loss: -5.559710 | norm:  0.701959 | fidelity:  0.994739 | Sz:  1.000002 | S^2:  2.000383
iter: 14 | loss: -5.570167 | norm:  0.506068 | fidelity:  0.996277 | Sz:  0.999999 | S^2:  2.000330
iter: 15 | loss: -5.575500 | norm:  0.318961 | fidelity:  0.996697 | Sz:  0.999999 | S^2:  2.000253
iter: 16 | loss: -5.573709 | norm:  0.324019 | fidelity:  0.995857 | Sz:  1.000000 | S^2:  2.000167
iter: 17 | loss: -5.567119 | norm:  0.488167 | fidelity:  0.994327 | Sz:  1.000001 | S^2:  2.000097
iter: 18 | loss: -5.560882 | norm:  0.620730 | fidelity:  0.993034 | Sz:  1.000001 | S^2:  2.000063
iter: 19 | loss: -5.559326 | norm:  0.654753 | fidelity:  0.992662 | Sz:  0.999999 | S^2:  2.000062
iter: 20 | loss: -5.563208 | norm:  0.579539 | fidelity:  0.993292 | Sz:  1.000000 | S^2:  2.000080
iter: 21 | loss: -5.569634 | norm:  0.419162 | fidelity:  0.994451 | Sz:  1.000000 | S^2:  2.000087
iter: 22 | loss: -5.574585 | norm:  0.240776 | fidelity:  0.995513 | Sz:  0.999998 | S^2:  2.000069
iter: 23 | loss: -5.575853 | norm:  0.213927 | fidelity:  0.996107 | Sz:  1.000000 | S^2:  2.000050
iter: 24 | loss: -5.574218 | norm:  0.333994 | fidelity:  0.996274 | Sz:  1.000001 | S^2:  2.000045
iter: 25 | loss: -5.572315 | norm:  0.415885 | fidelity:  0.996323 | Sz:  1.000001 | S^2:  2.000081
iter: 26 | loss: -5.572142 | norm:  0.413807 | fidelity:  0.996513 | Sz:  1.000000 | S^2:  2.000168
iter: 27 | loss: -5.573621 | norm:  0.339575 | fidelity:  0.996856 | Sz:  1.000001 | S^2:  2.000284
iter: 28 | loss: -5.575202 | norm:  0.241966 | fidelity:  0.997180 | Sz:  1.000000 | S^2:  2.000383
iter: 29 | loss: -5.575604 | norm:  0.207454 | fidelity:  0.997334 | Sz:  1.000000 | S^2:  2.000433
iter: 30 | loss: -5.574846 | norm:  0.257135 | fidelity:  0.997317 | Sz:  1.000001 | S^2:  2.000415
iter: 31 | loss: -5.573992 | norm:  0.305584 | fidelity:  0.997255 | Sz:  0.999998 | S^2:  2.000332
iter: 32 | loss: -5.574087 | norm:  0.309581 | fidelity:  0.997278 | Sz:  1.000000 | S^2:  2.000237
iter: 33 | loss: -5.575203 | norm:  0.268671 | fidelity:  0.997386 | Sz:  1.000000 | S^2:  2.000151
iter: 34 | loss: -5.576658 | norm:  0.203727 | fidelity:  0.997475 | Sz:  1.000002 | S^2:  2.000101
iter: 35 | loss: -5.577593 | norm:  0.149288 | fidelity:  0.997419 | Sz:  0.999998 | S^2:  2.000067
iter: 36 | loss: -5.577725 | norm:  0.141057 | fidelity:  0.997192 | Sz:  1.000001 | S^2:  2.000063
iter: 37 | loss: -5.577185 | norm:  0.169373 | fidelity:  0.996854 | Sz:  0.999999 | S^2:  2.000050
iter: 38 | loss: -5.576542 | norm:  0.198872 | fidelity:  0.996548 | Sz:  1.000002 | S^2:  2.000052
iter: 39 | loss: -5.576212 | norm:  0.211619 | fidelity:  0.996383 | Sz:  1.000001 | S^2:  2.000061
iter: 40 | loss: -5.576419 | norm:  0.200967 | fidelity:  0.996411 | Sz:  1.000000 | S^2:  2.000097
iter: 41 | loss: -5.576990 | norm:  0.168069 | fidelity:  0.996589 | Sz:  1.000000 | S^2:  2.000152
iter: 42 | loss: -5.577561 | norm:  0.126751 | fidelity:  0.996831 | Sz:  1.000002 | S^2:  2.000209
iter: 43 | loss: -5.577809 | norm:  0.107977 | fidelity:  0.997055 | Sz:  1.000002 | S^2:  2.000236
iter: 44 | loss: -5.577733 | norm:  0.124555 | fidelity:  0.997229 | Sz:  1.000000 | S^2:  2.000221
iter: 45 | loss: -5.577601 | norm:  0.142473 | fidelity:  0.997365 | Sz:  0.999998 | S^2:  2.000178
iter: 46 | loss: -5.577656 | norm:  0.137579 | fidelity:  0.997480 | Sz:  1.000000 | S^2:  2.000136
iter: 47 | loss: -5.577831 | norm:  0.110820 | fidelity:  0.997559 | Sz:  1.000000 | S^2:  2.000104
iter: 48 | loss: -5.577921 | norm:  0.086654 | fidelity:  0.997582 | Sz:  1.000000 | S^2:  2.000084
iter: 49 | loss: -5.577836 | norm:  0.095522 | fidelity:  0.997554 | Sz:  1.000001 | S^2:  2.000079
iter: 50 | loss: -5.577696 | norm:  0.117422 | fidelity:  0.997502 | Sz:  1.000001 | S^2:  2.000079
iter: 51 | loss: -5.577703 | norm:  0.121703 | fidelity:  0.997460 | Sz:  1.000000 | S^2:  2.000085
iter: 52 | loss: -5.577930 | norm:  0.101138 | fidelity:  0.997443 | Sz:  1.000002 | S^2:  2.000114
iter: 53 | loss: -5.578166 | norm:  0.066827 | fidelity:  0.997421 | Sz:  1.000000 | S^2:  2.000149
iter: 54 | loss: -5.578268 | norm:  0.050080 | fidelity:  0.997379 | Sz:  1.000001 | S^2:  2.000188
iter: 55 | loss: -5.578195 | norm:  0.067125 | fidelity:  0.997313 | Sz:  1.000001 | S^2:  2.000206
iter: 56 | loss: -5.578076 | norm:  0.082731 | fidelity:  0.997245 | Sz:  1.000000 | S^2:  2.000193
iter: 57 | loss: -5.578053 | norm:  0.083218 | fidelity:  0.997203 | Sz:  1.000002 | S^2:  2.000165
iter: 58 | loss: -5.578085 | norm:  0.073003 | fidelity:  0.997187 | Sz:  1.000000 | S^2:  2.000122
iter: 59 | loss: -5.578144 | norm:  0.062357 | fidelity:  0.997196 | Sz:  0.999999 | S^2:  2.000088
iter: 60 | loss: -5.578196 | norm:  0.057701 | fidelity:  0.997224 | Sz:  1.000001 | S^2:  2.000076
iter: 61 | loss: -5.578208 | norm:  0.056728 | fidelity:  0.997260 | Sz:  0.999999 | S^2:  2.000067
iter: 62 | loss: -5.578230 | norm:  0.055450 | fidelity:  0.997309 | Sz:  1.000001 | S^2:  2.000072
iter: 63 | loss: -5.578259 | norm:  0.052448 | fidelity:  0.997363 | Sz:  1.000002 | S^2:  2.000086
iter: 64 | loss: -5.578272 | norm:  0.047078 | fidelity:  0.997413 | Sz:  1.000000 | S^2:  2.000106
iter: 65 | loss: -5.578282 | norm:  0.040787 | fidelity:  0.997455 | Sz:  1.000000 | S^2:  2.000138
iter: 66 | loss: -5.578261 | norm:  0.039597 | fidelity:  0.997479 | Sz:  0.999998 | S^2:  2.000167
iter: 67 | loss: -5.578242 | norm:  0.045579 | fidelity:  0.997490 | Sz:  1.000000 | S^2:  2.000187
iter: 68 | loss: -5.578226 | norm:  0.050176 | fidelity:  0.997487 | Sz:  0.999999 | S^2:  2.000179
iter: 69 | loss: -5.578257 | norm:  0.045683 | fidelity:  0.997477 | Sz:  0.999999 | S^2:  2.000156
iter: 70 | loss: -5.578313 | norm:  0.031544 | fidelity:  0.997458 | Sz:  1.000000 | S^2:  2.000128
iter: 71 | loss: -5.578342 | norm:  0.018040 | fidelity:  0.997425 | Sz:  1.000000 | S^2:  2.000104
iter: 72 | loss: -5.578333 | norm:  0.025430 | fidelity:  0.997384 | Sz:  1.000001 | S^2:  2.000090
iter: 73 | loss: -5.578302 | norm:  0.036132 | fidelity:  0.997344 | Sz:  1.000000 | S^2:  2.000082
iter: 74 | loss: -5.578293 | norm:  0.037112 | fidelity:  0.997316 | Sz:  1.000000 | S^2:  2.000084
iter: 75 | loss: -5.578309 | norm:  0.029210 | fidelity:  0.997306 | Sz:  1.000000 | S^2:  2.000093
iter: 76 | loss: -5.578327 | norm:  0.021223 | fidelity:  0.997311 | Sz:  1.000000 | S^2:  2.000111
iter: 77 | loss: -5.578318 | norm:  0.023148 | fidelity:  0.997324 | Sz:  0.999998 | S^2:  2.000126
iter: 78 | loss: -5.578318 | norm:  0.027593 | fidelity:  0.997347 | Sz:  0.999999 | S^2:  2.000142
iter: 79 | loss: -5.578319 | norm:  0.026581 | fidelity:  0.997373 | Sz:  0.999999 | S^2:  2.000145
iter: 80 | loss: -5.578337 | norm:  0.020555 | fidelity:  0.997402 | Sz:  1.000000 | S^2:  2.000140
iter: 81 | loss: -5.578355 | norm:  0.014983 | fidelity:  0.997426 | Sz:  1.000002 | S^2:  2.000130
iter: 82 | loss: -5.578333 | norm:  0.015906 | fidelity:  0.997436 | Sz:  0.999998 | S^2:  2.000109
iter: 83 | loss: -5.578321 | norm:  0.020021 | fidelity:  0.997440 | Sz:  0.999998 | S^2:  2.000098
iter: 84 | loss: -5.578324 | norm:  0.022308 | fidelity:  0.997440 | Sz:  0.999999 | S^2:  2.000096
iter: 85 | loss: -5.578337 | norm:  0.020968 | fidelity:  0.997436 | Sz:  1.000001 | S^2:  2.000101
iter: 86 | loss: -5.578348 | norm:  0.015750 | fidelity:  0.997426 | Sz:  1.000001 | S^2:  2.000109
iter: 87 | loss: -5.578359 | norm:  0.008947 | fidelity:  0.997414 | Sz:  1.000001 | S^2:  2.000120

learning rate =  0.0009596105026192568
Find operators
1j [6^ 1^ 10 9] + -1j [10^ 9^ 6 1]
1j [6^ 5^ 10 1] + -1j [10^ 1^ 6 5]
1j [2^ 1^ 8 7] + -1j [8^ 7^ 2 1]
-1j [2^ 1^ 11 4] + 1j [11^ 4^ 2 1]
-1j [5^ 4^ 10 3] + 1j [10^ 3^ 5 4]
-1j [6^ 3^ 9 8] + 1j [9^ 8^ 6 3]
-1j [4^ 1^ 9 8] + 1j [9^ 8^ 4 1]
-1j [5^ 4^ 8 1] + 1j [8^ 1^ 5 4]
-1j [7^ 6^ 10 3] + 1j [10^ 3^ 7 6]
-1j [6^ 3^ 11 10] + 1j [11^ 10^ 6 3]
-1j [5^ 2^ 10 9] + 1j [10^ 9^ 5 2]
1j [6^ 5^ 9 2] + -1j [9^ 2^ 6 5]
with max gradients
[0.027640741, 0.027635608, 0.0206687, 0.020636456, 0.019706704, 0.01969205, 0.016410043, 0.016400615, 0.013736674, 0.013728738, 0.013201832, 0.013169349]

iter: 88 | loss: -5.578358 | norm:  0.067149 | fidelity:  0.997398 | Sz:  1.000001 | S^2:  2.000131
iter: 89 | loss: -5.578528 | norm:  0.076261 | fidelity:  0.997458 | Sz:  1.000002 | S^2:  2.000090
iter: 90 | loss: -5.578810 | norm:  0.065557 | fidelity:  0.997492 | Sz:  1.000007 | S^2:  2.000109
iter: 91 | loss: -5.578965 | norm:  0.065542 | fidelity:  0.997513 | Sz:  0.999999 | S^2:  2.000105
iter: 92 | loss: -5.579168 | norm:  0.068193 | fidelity:  0.997562 | Sz:  1.000002 | S^2:  2.000111
iter: 93 | loss: -5.579378 | norm:  0.065458 | fidelity:  0.997617 | Sz:  1.000002 | S^2:  2.000102
iter: 94 | loss: -5.579595 | norm:  0.061426 | fidelity:  0.997675 | Sz:  1.000003 | S^2:  2.000090
iter: 95 | loss: -5.579772 | norm:  0.060357 | fidelity:  0.997725 | Sz:  1.000000 | S^2:  2.000073
iter: 96 | loss: -5.579934 | norm:  0.061556 | fidelity:  0.997769 | Sz:  0.999996 | S^2:  2.000060
iter: 97 | loss: -5.580111 | norm:  0.061499 | fidelity:  0.997812 | Sz:  0.999995 | S^2:  2.000055
iter: 98 | loss: -5.580312 | norm:  0.059267 | fidelity:  0.997854 | Sz:  0.999998 | S^2:  2.000060
iter: 99 | loss: -5.580535 | norm:  0.056582 | fidelity:  0.997898 | Sz:  1.000005 | S^2:  2.000074
iter: 100 | loss: -5.580695 | norm:  0.055181 | fidelity:  0.997931 | Sz:  1.000002 | S^2:  2.000068
iter: 101 | loss: -5.580853 | norm:  0.054975 | fidelity:  0.997964 | Sz:  1.000000 | S^2:  2.000063
iter: 102 | loss: -5.581031 | norm:  0.054617 | fidelity:  0.998002 | Sz:  1.000002 | S^2:  2.000065
iter: 103 | loss: -5.581194 | norm:  0.053352 | fidelity:  0.998039 | Sz:  1.000002 | S^2:  2.000062
iter: 104 | loss: -5.581351 | norm:  0.051628 | fidelity:  0.998077 | Sz:  1.000001 | S^2:  2.000059
iter: 105 | loss: -5.581490 | norm:  0.050275 | fidelity:  0.998112 | Sz:  0.999998 | S^2:  2.000052
iter: 106 | loss: -5.581661 | norm:  0.049542 | fidelity:  0.998153 | Sz:  1.000001 | S^2:  2.000060
iter: 107 | loss: -5.581798 | norm:  0.048992 | fidelity:  0.998188 | Sz:  0.999999 | S^2:  2.000057
iter: 108 | loss: -5.581956 | norm:  0.048114 | fidelity:  0.998225 | Sz:  1.000002 | S^2:  2.000062
iter: 109 | loss: -5.582062 | norm:  0.046833 | fidelity:  0.998252 | Sz:  0.999995 | S^2:  2.000049
iter: 110 | loss: -5.582235 | norm:  0.045463 | fidelity:  0.998290 | Sz:  1.000002 | S^2:  2.000063
iter: 111 | loss: -5.582370 | norm:  0.044332 | fidelity:  0.998321 | Sz:  1.000002 | S^2:  2.000065
iter: 112 | loss: -5.582481 | norm:  0.043494 | fidelity:  0.998347 | Sz:  0.999999 | S^2:  2.000062
iter: 113 | loss: -5.582610 | norm:  0.042722 | fidelity:  0.998376 | Sz:  1.000000 | S^2:  2.000068
iter: 114 | loss: -5.582723 | norm:  0.041761 | fidelity:  0.998402 | Sz:  0.999999 | S^2:  2.000070
iter: 115 | loss: -5.582852 | norm:  0.040586 | fidelity:  0.998431 | Sz:  1.000001 | S^2:  2.000079
iter: 116 | loss: -5.582994 | norm:  0.039413 | fidelity:  0.998463 | Sz:  1.000006 | S^2:  2.000093
iter: 117 | loss: -5.583053 | norm:  0.038466 | fidelity:  0.998480 | Sz:  0.999997 | S^2:  2.000078
iter: 118 | loss: -5.583180 | norm:  0.037711 | fidelity:  0.998510 | Sz:  1.000001 | S^2:  2.000088
iter: 119 | loss: -5.583270 | norm:  0.036894 | fidelity:  0.998533 | Sz:  0.999999 | S^2:  2.000087
iter: 120 | loss: -5.583383 | norm:  0.035863 | fidelity:  0.998561 | Sz:  1.000002 | S^2:  2.000096
iter: 121 | loss: -5.583468 | norm:  0.034758 | fidelity:  0.998583 | Sz:  1.000000 | S^2:  2.000098
iter: 122 | loss: -5.583562 | norm:  0.033835 | fidelity:  0.998606 | Sz:  1.000000 | S^2:  2.000106
iter: 123 | loss: -5.583640 | norm:  0.033119 | fidelity:  0.998626 | Sz:  0.999999 | S^2:  2.000111
iter: 124 | loss: -5.583717 | norm:  0.032370 | fidelity:  0.998645 | Sz:  0.999998 | S^2:  2.000116
iter: 125 | loss: -5.583826 | norm:  0.031409 | fidelity:  0.998669 | Sz:  1.000003 | S^2:  2.000132
iter: 126 | loss: -5.583883 | norm:  0.030355 | fidelity:  0.998684 | Sz:  0.999999 | S^2:  2.000130
iter: 127 | loss: -5.583953 | norm:  0.029465 | fidelity:  0.998702 | Sz:  0.999998 | S^2:  2.000133
iter: 128 | loss: -5.584030 | norm:  0.028775 | fidelity:  0.998721 | Sz:  0.999999 | S^2:  2.000140
iter: 129 | loss: -5.584088 | norm:  0.028070 | fidelity:  0.998737 | Sz:  0.999998 | S^2:  2.000143
iter: 130 | loss: -5.584173 | norm:  0.027211 | fidelity:  0.998758 | Sz:  1.000001 | S^2:  2.000157
iter: 131 | loss: -5.584229 | norm:  0.026321 | fidelity:  0.998774 | Sz:  1.000000 | S^2:  2.000163
iter: 132 | loss: -5.584286 | norm:  0.025586 | fidelity:  0.998790 | Sz:  1.000000 | S^2:  2.000172
iter: 133 | loss: -5.584320 | norm:  0.024975 | fidelity:  0.998802 | Sz:  0.999996 | S^2:  2.000172
iter: 134 | loss: -5.584403 | norm:  0.024293 | fidelity:  0.998821 | Sz:  1.000001 | S^2:  2.000190
iter: 135 | loss: -5.584450 | norm:  0.023483 | fidelity:  0.998834 | Sz:  1.000000 | S^2:  2.000194
iter: 136 | loss: -5.584495 | norm:  0.022692 | fidelity:  0.998846 | Sz:  1.000000 | S^2:  2.000199
iter: 137 | loss: -5.584535 | norm:  0.022044 | fidelity:  0.998858 | Sz:  0.999998 | S^2:  2.000203
iter: 138 | loss: -5.584606 | norm:  0.021466 | fidelity:  0.998875 | Sz:  1.000003 | S^2:  2.000219
iter: 139 | loss: -5.584613 | norm:  0.020832 | fidelity:  0.998881 | Sz:  0.999997 | S^2:  2.000215
iter: 140 | loss: -5.584667 | norm:  0.020155 | fidelity:  0.998895 | Sz:  1.000000 | S^2:  2.000229
iter: 141 | loss: -5.584711 | norm:  0.019540 | fidelity:  0.998908 | Sz:  1.000001 | S^2:  2.000239
iter: 142 | loss: -5.584741 | norm:  0.019008 | fidelity:  0.998918 | Sz:  1.000000 | S^2:  2.000245
iter: 143 | loss: -5.584781 | norm:  0.018474 | fidelity:  0.998929 | Sz:  1.000001 | S^2:  2.000255
iter: 144 | loss: -5.584804 | norm:  0.017893 | fidelity:  0.998937 | Sz:  0.999999 | S^2:  2.000257
iter: 145 | loss: -5.584832 | norm:  0.017314 | fidelity:  0.998946 | Sz:  0.999999 | S^2:  2.000263
iter: 146 | loss: -5.584865 | norm:  0.016795 | fidelity:  0.998956 | Sz:  0.999999 | S^2:  2.000270
iter: 147 | loss: -5.584903 | norm:  0.016323 | fidelity:  0.998966 | Sz:  1.000001 | S^2:  2.000281
iter: 148 | loss: -5.584921 | norm:  0.015854 | fidelity:  0.998973 | Sz:  1.000000 | S^2:  2.000286
iter: 149 | loss: -5.584945 | norm:  0.015376 | fidelity:  0.998981 | Sz:  0.999999 | S^2:  2.000294
iter: 150 | loss: -5.584977 | norm:  0.014915 | fidelity:  0.998991 | Sz:  1.000001 | S^2:  2.000305
iter: 151 | loss: -5.585005 | norm:  0.014492 | fidelity:  0.999000 | Sz:  1.000002 | S^2:  2.000314
iter: 152 | loss: -5.585013 | norm:  0.014089 | fidelity:  0.999004 | Sz:  1.000000 | S^2:  2.000315
iter: 153 | loss: -5.585031 | norm:  0.013680 | fidelity:  0.999011 | Sz:  0.999999 | S^2:  2.000320
iter: 154 | loss: -5.585046 | norm:  0.013270 | fidelity:  0.999017 | Sz:  0.999999 | S^2:  2.000324
iter: 155 | loss: -5.585075 | norm:  0.012887 | fidelity:  0.999025 | Sz:  1.000001 | S^2:  2.000334
iter: 156 | loss: -5.585097 | norm:  0.012542 | fidelity:  0.999032 | Sz:  1.000001 | S^2:  2.000342
iter: 157 | loss: -5.585112 | norm:  0.012206 | fidelity:  0.999038 | Sz:  1.000001 | S^2:  2.000349
iter: 158 | loss: -5.585109 | norm:  0.011862 | fidelity:  0.999040 | Sz:  0.999998 | S^2:  2.000349
iter: 159 | loss: -5.585118 | norm:  0.011527 | fidelity:  0.999045 | Sz:  0.999997 | S^2:  2.000353
iter: 160 | loss: -5.585162 | norm:  0.011220 | fidelity:  0.999056 | Sz:  1.000002 | S^2:  2.000368
iter: 161 | loss: -5.585164 | norm:  0.010928 | fidelity:  0.999059 | Sz:  1.000000 | S^2:  2.000368
iter: 162 | loss: -5.585173 | norm:  0.010633 | fidelity:  0.999063 | Sz:  1.000000 | S^2:  2.000372
iter: 163 | loss: -5.585181 | norm:  0.010340 | fidelity:  0.999067 | Sz:  0.999999 | S^2:  2.000376
iter: 164 | loss: -5.585197 | norm:  0.010072 | fidelity:  0.999072 | Sz:  1.000000 | S^2:  2.000383
iter: 165 | loss: -5.585212 | norm:  0.009824 | fidelity:  0.999078 | Sz:  1.000001 | S^2:  2.000390

learning rate =  0.0006593763155507522
Find operators
1j [5^ 0^ 11 10] + -1j [11^ 10^ 5 0]
-1j [7^ 6^ 9 0] + 1j [9^ 0^ 7 6]
-1j [6^ 1^ 11 8] + 1j [11^ 8^ 6 1]
-1j [7^ 4^ 10 1] + 1j [10^ 1^ 7 4]
-1j [9^ 4^ 10 7] + 1j [10^ 7^ 9 4]
1j [8^ 5^ 11 6] + -1j [11^ 6^ 8 5]
1j [5^ 4^ 7 6] + -1j [7^ 6^ 5 4]
1j [9^ 8^ 11 10] + -1j [11^ 10^ 9 8]
with max gradients
[0.01604149, 0.016032755, 0.013439655, 0.013435216, 0.0116784405, 0.011671328, 0.011030427, 0.011022817]

iter: 166 | loss: -5.585204 | norm:  0.038509 | fidelity:  0.999079 | Sz:  0.999998 | S^2:  2.000389
iter: 167 | loss: -5.585271 | norm:  0.048695 | fidelity:  0.999095 | Sz:  1.000000 | S^2:  2.000335
iter: 168 | loss: -5.585389 | norm:  0.038124 | fidelity:  0.999108 | Sz:  1.000001 | S^2:  2.000385
iter: 169 | loss: -5.585436 | norm:  0.039088 | fidelity:  0.999111 | Sz:  0.999996 | S^2:  2.000427
iter: 170 | loss: -5.585515 | norm:  0.042792 | fidelity:  0.999126 | Sz:  0.999998 | S^2:  2.000458
iter: 171 | loss: -5.585631 | norm:  0.040910 | fidelity:  0.999148 | Sz:  1.000004 | S^2:  2.000468
iter: 172 | loss: -5.585686 | norm:  0.036656 | fidelity:  0.999159 | Sz:  0.999999 | S^2:  2.000437
iter: 173 | loss: -5.585761 | norm:  0.034333 | fidelity:  0.999174 | Sz:  0.999998 | S^2:  2.000410
iter: 174 | loss: -5.585832 | norm:  0.035254 | fidelity:  0.999187 | Sz:  0.999998 | S^2:  2.000386
iter: 175 | loss: -5.585886 | norm:  0.036622 | fidelity:  0.999196 | Sz:  0.999996 | S^2:  2.000366
iter: 176 | loss: -5.585960 | norm:  0.036038 | fidelity:  0.999208 | Sz:  0.999997 | S^2:  2.000363
iter: 177 | loss: -5.586034 | norm:  0.033774 | fidelity:  0.999218 | Sz:  0.999997 | S^2:  2.000368
iter: 178 | loss: -5.586130 | norm:  0.031558 | fidelity:  0.999233 | Sz:  1.000002 | S^2:  2.000387
iter: 179 | loss: -5.586195 | norm:  0.030877 | fidelity:  0.999243 | Sz:  1.000002 | S^2:  2.000398
iter: 180 | loss: -5.586238 | norm:  0.031499 | fidelity:  0.999250 | Sz:  0.999999 | S^2:  2.000401
iter: 181 | loss: -5.586323 | norm:  0.031919 | fidelity:  0.999265 | Sz:  1.000003 | S^2:  2.000412
iter: 182 | loss: -5.586357 | norm:  0.031179 | fidelity:  0.999271 | Sz:  0.999998 | S^2:  2.000397
iter: 183 | loss: -5.586431 | norm:  0.029579 | fidelity:  0.999285 | Sz:  1.000001 | S^2:  2.000390
iter: 184 | loss: -5.586508 | norm:  0.028156 | fidelity:  0.999299 | Sz:  1.000004 | S^2:  2.000382
iter: 185 | loss: -5.586554 | norm:  0.027688 | fidelity:  0.999308 | Sz:  1.000002 | S^2:  2.000363
iter: 186 | loss: -5.586599 | norm:  0.027920 | fidelity:  0.999316 | Sz:  1.000001 | S^2:  2.000349
iter: 187 | loss: -5.586653 | norm:  0.027956 | fidelity:  0.999326 | Sz:  1.000002 | S^2:  2.000343
iter: 188 | loss: -5.586705 | norm:  0.027280 | fidelity:  0.999335 | Sz:  1.000002 | S^2:  2.000343
iter: 189 | loss: -5.586743 | norm:  0.026150 | fidelity:  0.999341 | Sz:  1.000000 | S^2:  2.000342
iter: 190 | loss: -5.586799 | norm:  0.025254 | fidelity:  0.999351 | Sz:  1.000001 | S^2:  2.000351
iter: 191 | loss: -5.586833 | norm:  0.024998 | fidelity:  0.999357 | Sz:  0.999999 | S^2:  2.000354
iter: 192 | loss: -5.586879 | norm:  0.025102 | fidelity:  0.999366 | Sz:  0.999999 | S^2:  2.000359
iter: 193 | loss: -5.586932 | norm:  0.024975 | fidelity:  0.999375 | Sz:  1.000001 | S^2:  2.000363
iter: 194 | loss: -5.586962 | norm:  0.024366 | fidelity:  0.999381 | Sz:  0.999999 | S^2:  2.000355
iter: 195 | loss: -5.587009 | norm:  0.023547 | fidelity:  0.999390 | Sz:  1.000000 | S^2:  2.000350
iter: 196 | loss: -5.587051 | norm:  0.022983 | fidelity:  0.999399 | Sz:  1.000000 | S^2:  2.000343
iter: 197 | loss: -5.587090 | norm:  0.022829 | fidelity:  0.999406 | Sz:  1.000000 | S^2:  2.000335
iter: 198 | loss: -5.587133 | norm:  0.022801 | fidelity:  0.999415 | Sz:  1.000002 | S^2:  2.000332
iter: 199 | loss: -5.587165 | norm:  0.022549 | fidelity:  0.999421 | Sz:  1.000001 | S^2:  2.000327
iter: 200 | loss: -5.587189 | norm:  0.022039 | fidelity:  0.999426 | Sz:  0.999999 | S^2:  2.000323
iter: 201 | loss: -5.587234 | norm:  0.021542 | fidelity:  0.999435 | Sz:  1.000001 | S^2:  2.000329
iter: 202 | loss: -5.587277 | norm:  0.021301 | fidelity:  0.999443 | Sz:  1.000002 | S^2:  2.000335
iter: 203 | loss: -5.587294 | norm:  0.021257 | fidelity:  0.999446 | Sz:  1.000000 | S^2:  2.000331
iter: 204 | loss: -5.587314 | norm:  0.021154 | fidelity:  0.999450 | Sz:  0.999998 | S^2:  2.000326
iter: 205 | loss: -5.587353 | norm:  0.020849 | fidelity:  0.999458 | Sz:  0.999999 | S^2:  2.000325
iter: 206 | loss: -5.587373 | norm:  0.020456 | fidelity:  0.999462 | Sz:  0.999998 | S^2:  2.000316
iter: 207 | loss: -5.587421 | norm:  0.020181 | fidelity:  0.999472 | Sz:  1.000001 | S^2:  2.000317
iter: 208 | loss: -5.587438 | norm:  0.020080 | fidelity:  0.999475 | Sz:  0.999999 | S^2:  2.000308
iter: 209 | loss: -5.587495 | norm:  0.020015 | fidelity:  0.999487 | Sz:  1.000005 | S^2:  2.000315
iter: 210 | loss: -5.587520 | norm:  0.019848 | fidelity:  0.999492 | Sz:  1.000004 | S^2:  2.000313
iter: 211 | loss: -5.587518 | norm:  0.019601 | fidelity:  0.999492 | Sz:  0.999999 | S^2:  2.000302
iter: 212 | loss: -5.587551 | norm:  0.019404 | fidelity:  0.999499 | Sz:  1.000000 | S^2:  2.000306
iter: 213 | loss: -5.587555 | norm:  0.019321 | fidelity:  0.999501 | Sz:  0.999997 | S^2:  2.000299
iter: 214 | loss: -5.587608 | norm:  0.019276 | fidelity:  0.999511 | Sz:  1.000002 | S^2:  2.000309
iter: 215 | loss: -5.587624 | norm:  0.019164 | fidelity:  0.999514 | Sz:  1.000001 | S^2:  2.000305
iter: 216 | loss: -5.587643 | norm:  0.018981 | fidelity:  0.999518 | Sz:  1.000000 | S^2:  2.000299
iter: 217 | loss: -5.587688 | norm:  0.018816 | fidelity:  0.999527 | Sz:  1.000004 | S^2:  2.000303
iter: 218 | loss: -5.587696 | norm:  0.018726 | fidelity:  0.999530 | Sz:  1.000001 | S^2:  2.000294
iter: 219 | loss: -5.587713 | norm:  0.018673 | fidelity:  0.999534 | Sz:  1.000000 | S^2:  2.000288
iter: 220 | loss: -5.587747 | norm:  0.018588 | fidelity:  0.999541 | Sz:  1.000002 | S^2:  2.000290
iter: 221 | loss: -5.587752 | norm:  0.018464 | fidelity:  0.999542 | Sz:  0.999999 | S^2:  2.000283
iter: 222 | loss: -5.587770 | norm:  0.018356 | fidelity:  0.999546 | Sz:  0.999999 | S^2:  2.000281
iter: 223 | loss: -5.587776 | norm:  0.018294 | fidelity:  0.999548 | Sz:  0.999996 | S^2:  2.000274
iter: 224 | loss: -5.587826 | norm:  0.018247 | fidelity:  0.999557 | Sz:  1.000001 | S^2:  2.000283
iter: 225 | loss: -5.587826 | norm:  0.018170 | fidelity:  0.999558 | Sz:  0.999998 | S^2:  2.000273
iter: 226 | loss: -5.587856 | norm:  0.018064 | fidelity:  0.999564 | Sz:  1.000000 | S^2:  2.000274
iter: 227 | loss: -5.587879 | norm:  0.017966 | fidelity:  0.999569 | Sz:  1.000000 | S^2:  2.000271
iter: 228 | loss: -5.587895 | norm:  0.017899 | fidelity:  0.999573 | Sz:  0.999999 | S^2:  2.000266
iter: 229 | loss: -5.587914 | norm:  0.017840 | fidelity:  0.999577 | Sz:  0.999999 | S^2:  2.000263
iter: 230 | loss: -5.587950 | norm:  0.017765 | fidelity:  0.999584 | Sz:  1.000002 | S^2:  2.000267
iter: 231 | loss: -5.587946 | norm:  0.017680 | fidelity:  0.999584 | Sz:  0.999998 | S^2:  2.000257
iter: 232 | loss: -5.587975 | norm:  0.017609 | fidelity:  0.999590 | Sz:  1.000000 | S^2:  2.000260
iter: 233 | loss: -5.588000 | norm:  0.017554 | fidelity:  0.999595 | Sz:  1.000001 | S^2:  2.000261
iter: 234 | loss: -5.588021 | norm:  0.017495 | fidelity:  0.999599 | Sz:  1.000001 | S^2:  2.000260
iter: 235 | loss: -5.588028 | norm:  0.017417 | fidelity:  0.999601 | Sz:  0.999999 | S^2:  2.000253
iter: 236 | loss: -5.588042 | norm:  0.017332 | fidelity:  0.999604 | Sz:  0.999999 | S^2:  2.000248
iter: 237 | loss: -5.588080 | norm:  0.017257 | fidelity:  0.999611 | Sz:  1.000002 | S^2:  2.000252
iter: 238 | loss: -5.588103 | norm:  0.017191 | fidelity:  0.999616 | Sz:  1.000003 | S^2:  2.000251
iter: 239 | loss: -5.588108 | norm:  0.017121 | fidelity:  0.999617 | Sz:  1.000001 | S^2:  2.000244
iter: 240 | loss: -5.588136 | norm:  0.017046 | fidelity:  0.999623 | Sz:  1.000003 | S^2:  2.000246
iter: 241 | loss: -5.588141 | norm:  0.016973 | fidelity:  0.999624 | Sz:  1.000000 | S^2:  2.000239
iter: 242 | loss: -5.588144 | norm:  0.016909 | fidelity:  0.999625 | Sz:  0.999998 | S^2:  2.000232
iter: 243 | loss: -5.588160 | norm:  0.016843 | fidelity:  0.999629 | Sz:  0.999998 | S^2:  2.000229
iter: 244 | loss: -5.588186 | norm:  0.016769 | fidelity:  0.999634 | Sz:  0.999999 | S^2:  2.000230
iter: 245 | loss: -5.588222 | norm:  0.016690 | fidelity:  0.999641 | Sz:  1.000002 | S^2:  2.000234
iter: 246 | loss: -5.588246 | norm:  0.016613 | fidelity:  0.999646 | Sz:  1.000004 | S^2:  2.000234
iter: 247 | loss: -5.588254 | norm:  0.016541 | fidelity:  0.999647 | Sz:  1.000002 | S^2:  2.000228
iter: 248 | loss: -5.588266 | norm:  0.016469 | fidelity:  0.999650 | Sz:  1.000001 | S^2:  2.000224
iter: 249 | loss: -5.588276 | norm:  0.016394 | fidelity:  0.999652 | Sz:  1.000000 | S^2:  2.000220
iter: 250 | loss: -5.588285 | norm:  0.016320 | fidelity:  0.999654 | Sz:  0.999999 | S^2:  2.000215
iter: 251 | loss: -5.588302 | norm:  0.016250 | fidelity:  0.999658 | Sz:  0.999999 | S^2:  2.000213
iter: 252 | loss: -5.588307 | norm:  0.016179 | fidelity:  0.999659 | Sz:  0.999997 | S^2:  2.000207
iter: 253 | loss: -5.588332 | norm:  0.016103 | fidelity:  0.999664 | Sz:  0.999999 | S^2:  2.000208
iter: 254 | loss: -5.588360 | norm:  0.016025 | fidelity:  0.999669 | Sz:  1.000001 | S^2:  2.000210
iter: 255 | loss: -5.588372 | norm:  0.015948 | fidelity:  0.999672 | Sz:  1.000000 | S^2:  2.000206
iter: 256 | loss: -5.588393 | norm:  0.015873 | fidelity:  0.999676 | Sz:  1.000001 | S^2:  2.000205
iter: 257 | loss: -5.588412 | norm:  0.015797 | fidelity:  0.999680 | Sz:  1.000002 | S^2:  2.000204
iter: 258 | loss: -5.588425 | norm:  0.015721 | fidelity:  0.999682 | Sz:  1.000001 | S^2:  2.000201
iter: 259 | loss: -5.588427 | norm:  0.015646 | fidelity:  0.999683 | Sz:  0.999999 | S^2:  2.000194
iter: 260 | loss: -5.588449 | norm:  0.015573 | fidelity:  0.999687 | Sz:  1.000000 | S^2:  2.000195
iter: 261 | loss: -5.588466 | norm:  0.015497 | fidelity:  0.999691 | Sz:  1.000000 | S^2:  2.000193
iter: 262 | loss: -5.588488 | norm:  0.015420 | fidelity:  0.999695 | Sz:  1.000002 | S^2:  2.000194
iter: 263 | loss: -5.588488 | norm:  0.015342 | fidelity:  0.999695 | Sz:  0.999999 | S^2:  2.000186
iter: 264 | loss: -5.588520 | norm:  0.015266 | fidelity:  0.999701 | Sz:  1.000002 | S^2:  2.000190
iter: 265 | loss: -5.588531 | norm:  0.015189 | fidelity:  0.999704 | Sz:  1.000002 | S^2:  2.000186
iter: 266 | loss: -5.588545 | norm:  0.015112 | fidelity:  0.999706 | Sz:  1.000002 | S^2:  2.000185
iter: 267 | loss: -5.588549 | norm:  0.015036 | fidelity:  0.999707 | Sz:  1.000000 | S^2:  2.000179
iter: 268 | loss: -5.588562 | norm:  0.014960 | fidelity:  0.999710 | Sz:  1.000000 | S^2:  2.000177
iter: 269 | loss: -5.588581 | norm:  0.014884 | fidelity:  0.999714 | Sz:  1.000000 | S^2:  2.000176
iter: 270 | loss: -5.588604 | norm:  0.014807 | fidelity:  0.999718 | Sz:  1.000002 | S^2:  2.000178
iter: 271 | loss: -5.588601 | norm:  0.014729 | fidelity:  0.999718 | Sz:  0.999999 | S^2:  2.000169
iter: 272 | loss: -5.588623 | norm:  0.014652 | fidelity:  0.999722 | Sz:  1.000001 | S^2:  2.000170
iter: 273 | loss: -5.588629 | norm:  0.014574 | fidelity:  0.999724 | Sz:  0.999999 | S^2:  2.000166
iter: 274 | loss: -5.588646 | norm:  0.014497 | fidelity:  0.999727 | Sz:  1.000000 | S^2:  2.000165
iter: 275 | loss: -5.588657 | norm:  0.014420 | fidelity:  0.999729 | Sz:  0.999999 | S^2:  2.000162
iter: 276 | loss: -5.588673 | norm:  0.014343 | fidelity:  0.999732 | Sz:  1.000000 | S^2:  2.000161
iter: 277 | loss: -5.588700 | norm:  0.014267 | fidelity:  0.999737 | Sz:  1.000003 | S^2:  2.000164
iter: 278 | loss: -5.588706 | norm:  0.014189 | fidelity:  0.999738 | Sz:  1.000001 | S^2:  2.000160
iter: 279 | loss: -5.588701 | norm:  0.014112 | fidelity:  0.999738 | Sz:  0.999998 | S^2:  2.000152
iter: 280 | loss: -5.588718 | norm:  0.014034 | fidelity:  0.999741 | Sz:  0.999999 | S^2:  2.000151
iter: 281 | loss: -5.588725 | norm:  0.013956 | fidelity:  0.999743 | Sz:  0.999998 | S^2:  2.000147
iter: 282 | loss: -5.588747 | norm:  0.013879 | fidelity:  0.999747 | Sz:  1.000000 | S^2:  2.000149
iter: 283 | loss: -5.588753 | norm:  0.013802 | fidelity:  0.999748 | Sz:  0.999998 | S^2:  2.000145
iter: 284 | loss: -5.588753 | norm:  0.013725 | fidelity:  0.999748 | Sz:  0.999996 | S^2:  2.000139
iter: 285 | loss: -5.588775 | norm:  0.013649 | fidelity:  0.999753 | Sz:  0.999998 | S^2:  2.000141
iter: 286 | loss: -5.588787 | norm:  0.013572 | fidelity:  0.999755 | Sz:  0.999998 | S^2:  2.000139
iter: 287 | loss: -5.588814 | norm:  0.013495 | fidelity:  0.999760 | Sz:  1.000001 | S^2:  2.000142
iter: 288 | loss: -5.588820 | norm:  0.013417 | fidelity:  0.999761 | Sz:  1.000000 | S^2:  2.000139
iter: 289 | loss: -5.588827 | norm:  0.013340 | fidelity:  0.999763 | Sz:  0.999999 | S^2:  2.000135
iter: 290 | loss: -5.588844 | norm:  0.013264 | fidelity:  0.999766 | Sz:  1.000000 | S^2:  2.000135
iter: 291 | loss: -5.588861 | norm:  0.013187 | fidelity:  0.999769 | Sz:  1.000001 | S^2:  2.000136
iter: 292 | loss: -5.588862 | norm:  0.013111 | fidelity:  0.999770 | Sz:  0.999999 | S^2:  2.000130
iter: 293 | loss: -5.588868 | norm:  0.013035 | fidelity:  0.999771 | Sz:  0.999998 | S^2:  2.000127
iter: 294 | loss: -5.588870 | norm:  0.012959 | fidelity:  0.999771 | Sz:  0.999997 | S^2:  2.000122
iter: 295 | loss: -5.588904 | norm:  0.012883 | fidelity:  0.999778 | Sz:  1.000001 | S^2:  2.000128
iter: 296 | loss: -5.588916 | norm:  0.012806 | fidelity:  0.999780 | Sz:  1.000001 | S^2:  2.000127
iter: 297 | loss: -5.588924 | norm:  0.012730 | fidelity:  0.999782 | Sz:  1.000000 | S^2:  2.000125
iter: 298 | loss: -5.588936 | norm:  0.012655 | fidelity:  0.999784 | Sz:  1.000001 | S^2:  2.000124
iter: 299 | loss: -5.588944 | norm:  0.012579 | fidelity:  0.999786 | Sz:  1.000000 | S^2:  2.000121
iter: 300 | loss: -5.588961 | norm:  0.012504 | fidelity:  0.999789 | Sz:  1.000001 | S^2:  2.000122
iter: 301 | loss: -5.588963 | norm:  0.012429 | fidelity:  0.999789 | Sz:  1.000000 | S^2:  2.000117
iter: 302 | loss: -5.588959 | norm:  0.012354 | fidelity:  0.999789 | Sz:  0.999997 | S^2:  2.000111
iter: 303 | loss: -5.588999 | norm:  0.012279 | fidelity:  0.999796 | Sz:  1.000003 | S^2:  2.000120
iter: 304 | loss: -5.588995 | norm:  0.012204 | fidelity:  0.999796 | Sz:  1.000000 | S^2:  2.000113
iter: 305 | loss: -5.589007 | norm:  0.012129 | fidelity:  0.999798 | Sz:  1.000001 | S^2:  2.000113
iter: 306 | loss: -5.589014 | norm:  0.012055 | fidelity:  0.999800 | Sz:  1.000000 | S^2:  2.000110
iter: 307 | loss: -5.589005 | norm:  0.011981 | fidelity:  0.999798 | Sz:  0.999997 | S^2:  2.000102
iter: 308 | loss: -5.589044 | norm:  0.011907 | fidelity:  0.999805 | Sz:  1.000002 | S^2:  2.000111
iter: 309 | loss: -5.589029 | norm:  0.011833 | fidelity:  0.999803 | Sz:  0.999998 | S^2:  2.000101
iter: 310 | loss: -5.589057 | norm:  0.011759 | fidelity:  0.999808 | Sz:  1.000001 | S^2:  2.000106
iter: 311 | loss: -5.589058 | norm:  0.011686 | fidelity:  0.999808 | Sz:  1.000000 | S^2:  2.000102
iter: 312 | loss: -5.589075 | norm:  0.011613 | fidelity:  0.999811 | Sz:  1.000001 | S^2:  2.000103
iter: 313 | loss: -5.589076 | norm:  0.011539 | fidelity:  0.999812 | Sz:  0.999999 | S^2:  2.000099
iter: 314 | loss: -5.589081 | norm:  0.011466 | fidelity:  0.999813 | Sz:  0.999999 | S^2:  2.000096
iter: 315 | loss: -5.589097 | norm:  0.011394 | fidelity:  0.999816 | Sz:  1.000000 | S^2:  2.000097
iter: 316 | loss: -5.589108 | norm:  0.011321 | fidelity:  0.999818 | Sz:  1.000000 | S^2:  2.000097
iter: 317 | loss: -5.589102 | norm:  0.011249 | fidelity:  0.999817 | Sz:  0.999998 | S^2:  2.000090
iter: 318 | loss: -5.589122 | norm:  0.011177 | fidelity:  0.999821 | Sz:  1.000000 | S^2:  2.000093
iter: 319 | loss: -5.589127 | norm:  0.011105 | fidelity:  0.999822 | Sz:  0.999999 | S^2:  2.000090
iter: 320 | loss: -5.589131 | norm:  0.011033 | fidelity:  0.999823 | Sz:  0.999998 | S^2:  2.000087
iter: 321 | loss: -5.589150 | norm:  0.010961 | fidelity:  0.999826 | Sz:  1.000000 | S^2:  2.000090
iter: 322 | loss: -5.589159 | norm:  0.010890 | fidelity:  0.999828 | Sz:  1.000000 | S^2:  2.000089
iter: 323 | loss: -5.589178 | norm:  0.010819 | fidelity:  0.999831 | Sz:  1.000002 | S^2:  2.000092
iter: 324 | loss: -5.589169 | norm:  0.010748 | fidelity:  0.999830 | Sz:  0.999999 | S^2:  2.000085
iter: 325 | loss: -5.589193 | norm:  0.010677 | fidelity:  0.999834 | Sz:  1.000002 | S^2:  2.000089
iter: 326 | loss: -5.589184 | norm:  0.010607 | fidelity:  0.999833 | Sz:  0.999999 | S^2:  2.000082
iter: 327 | loss: -5.589185 | norm:  0.010537 | fidelity:  0.999833 | Sz:  0.999998 | S^2:  2.000078
iter: 328 | loss: -5.589213 | norm:  0.010466 | fidelity:  0.999838 | Sz:  1.000002 | S^2:  2.000084
iter: 329 | loss: -5.589213 | norm:  0.010397 | fidelity:  0.999839 | Sz:  1.000000 | S^2:  2.000080
iter: 330 | loss: -5.589223 | norm:  0.010327 | fidelity:  0.999841 | Sz:  1.000001 | S^2:  2.000080
iter: 331 | loss: -5.589233 | norm:  0.010257 | fidelity:  0.999842 | Sz:  1.000001 | S^2:  2.000080
iter: 332 | loss: -5.589243 | norm:  0.010188 | fidelity:  0.999844 | Sz:  1.000002 | S^2:  2.000080
iter: 333 | loss: -5.589241 | norm:  0.010119 | fidelity:  0.999844 | Sz:  1.000000 | S^2:  2.000075
iter: 334 | loss: -5.589260 | norm:  0.010050 | fidelity:  0.999848 | Sz:  1.000002 | S^2:  2.000078
iter: 335 | loss: -5.589257 | norm:  0.009982 | fidelity:  0.999847 | Sz:  1.000000 | S^2:  2.000074

learning rate =  0.0006278757471591235
Find operators
-1j [7^ 6^ 8 1] + 1j [8^ 1^ 7 6]
-1j [4^ 1^ 11 10] + 1j [11^ 10^ 4 1]
1j [4^ 3^ 10 9] + -1j [10^ 9^ 4 3]
1j [6^ 5^ 8 3] + -1j [8^ 3^ 6 5]
with max gradients
[0.012854897, 0.012843222, 0.012277018, 0.012241029]

iter: 336 | loss: -5.589270 | norm:  0.027001 | fidelity:  0.999850 | Sz:  1.000001 | S^2:  2.000075
iter: 337 | loss: -5.589293 | norm:  0.037051 | fidelity:  0.999859 | Sz:  1.000002 | S^2:  2.000048
iter: 338 | loss: -5.589323 | norm:  0.025927 | fidelity:  0.999864 | Sz:  0.999999 | S^2:  2.000058
iter: 339 | loss: -5.589368 | norm:  0.028036 | fidelity:  0.999871 | Sz:  1.000002 | S^2:  2.000080
iter: 340 | loss: -5.589374 | norm:  0.029445 | fidelity:  0.999872 | Sz:  0.999998 | S^2:  2.000078
iter: 341 | loss: -5.589411 | norm:  0.024663 | fidelity:  0.999879 | Sz:  0.999999 | S^2:  2.000072
iter: 342 | loss: -5.589454 | norm:  0.018607 | fidelity:  0.999888 | Sz:  1.000001 | S^2:  2.000062
iter: 343 | loss: -5.589482 | norm:  0.016822 | fidelity:  0.999894 | Sz:  1.000001 | S^2:  2.000048
iter: 344 | loss: -5.589505 | norm:  0.019317 | fidelity:  0.999900 | Sz:  1.000001 | S^2:  2.000037
iter: 345 | loss: -5.589516 | norm:  0.020909 | fidelity:  0.999903 | Sz:  0.999999 | S^2:  2.000026
iter: 346 | loss: -5.589534 | norm:  0.019421 | fidelity:  0.999907 | Sz:  0.999999 | S^2:  2.000021
iter: 347 | loss: -5.589545 | norm:  0.015951 | fidelity:  0.999909 | Sz:  0.999996 | S^2:  2.000015
iter: 348 | loss: -5.589593 | norm:  0.013130 | fidelity:  0.999918 | Sz:  1.000001 | S^2:  2.000025
iter: 349 | loss: -5.589614 | norm:  0.013020 | fidelity:  0.999922 | Sz:  1.000002 | S^2:  2.000028
iter: 350 | loss: -5.589619 | norm:  0.014264 | fidelity:  0.999923 | Sz:  1.000000 | S^2:  2.000026
iter: 351 | loss: -5.589613 | norm:  0.014448 | fidelity:  0.999922 | Sz:  0.999997 | S^2:  2.000019
iter: 352 | loss: -5.589649 | norm:  0.012845 | fidelity:  0.999928 | Sz:  1.000000 | S^2:  2.000024
iter: 353 | loss: -5.589669 | norm:  0.010207 | fidelity:  0.999932 | Sz:  1.000002 | S^2:  2.000022
iter: 354 | loss: -5.589674 | norm:  0.008173 | fidelity:  0.999933 | Sz:  1.000000 | S^2:  2.000015


convergence criterion has satisfied, break the loop!
total run time:  10296.261474132538
