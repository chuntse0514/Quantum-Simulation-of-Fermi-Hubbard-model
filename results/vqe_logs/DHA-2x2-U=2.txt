spin up orbital energies: {0: -2.0, 2: 0, 4: 0, 6: 2.0}
spin down orbital energies:  {1: -2.0, 3: 0, 5: 0, 7: 2.0}
spin up indices:  [0, 2]    spin down indices:  [1, 3] 

ground state energy:  -2.8284271247461934
particle number:  4

learning rate =  0.04999999914428645
Find operators
1j [1^ 0^ 5 4] + -1j [5^ 4^ 1 0]
1j [1^ 0^ 7 6] + -1j [7^ 6^ 1 0]
1j [3^ 0^ 7 4] + -1j [7^ 4^ 3 0]
1j [3^ 2^ 5 4] + -1j [5^ 4^ 3 2]
1j [3^ 2^ 7 6] + -1j [7^ 6^ 3 2]
-1j [2^ 1^ 7 4] + 1j [7^ 4^ 2 1]
1j [2^ 1^ 6 5] + -1j [6^ 5^ 2 1]
-1j [3^ 0^ 6 5] + 1j [6^ 5^ 3 0]
with max gradients
[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]

iter: 1 | loss: -2.000000 | norm:  2.828427 | fidelity:  0.470971 | Sz:  0.000000 | S^2: -0.000000
iter: 2 | loss: -2.317278 | norm:  1.736686 | fidelity:  0.542559 | Sz: -0.000000 | S^2:  0.000000
iter: 3 | loss: -2.465790 | norm:  1.111688 | fidelity:  0.597410 | Sz: -0.000000 | S^2:  0.000020
iter: 4 | loss: -2.498872 | norm:  1.183169 | fidelity:  0.636896 | Sz:  0.000000 | S^2:  0.000135
iter: 5 | loss: -2.490165 | norm:  1.412866 | fidelity:  0.669190 | Sz: -0.000000 | S^2:  0.000284
iter: 6 | loss: -2.494238 | norm:  1.500298 | fidelity:  0.702527 | Sz: -0.000000 | S^2:  0.000256
iter: 7 | loss: -2.526797 | norm:  1.433380 | fidelity:  0.739776 | Sz:  0.000000 | S^2:  0.000098
iter: 8 | loss: -2.577254 | norm:  1.279221 | fidelity:  0.779562 | Sz:  0.000000 | S^2:  0.000004
iter: 9 | loss: -2.631566 | norm:  1.090153 | fidelity:  0.819355 | Sz: -0.000000 | S^2:  0.000047
iter: 10 | loss: -2.680066 | norm:  0.890111 | fidelity:  0.856776 | Sz: -0.000000 | S^2:  0.000125
iter: 11 | loss: -2.716704 | norm:  0.712797 | fidelity:  0.889891 | Sz:  0.000000 | S^2:  0.000098
iter: 12 | loss: -2.739562 | norm:  0.614199 | fidelity:  0.917586 | Sz:  0.000000 | S^2:  0.000016
iter: 13 | loss: -2.751203 | norm:  0.619048 | fidelity:  0.939718 | Sz: -0.000000 | S^2:  0.000344
iter: 14 | loss: -2.756439 | norm:  0.678976 | fidelity:  0.956774 | Sz:  0.000000 | S^2:  0.001913
iter: 15 | loss: -2.759840 | norm:  0.727262 | fidelity:  0.969466 | Sz: -0.000000 | S^2:  0.005550
iter: 16 | loss: -2.764460 | norm:  0.721243 | fidelity:  0.978525 | Sz: -0.000000 | S^2:  0.011715
iter: 17 | loss: -2.770852 | norm:  0.647776 | fidelity:  0.984514 | Sz: -0.000000 | S^2:  0.020386
iter: 18 | loss: -2.777222 | norm:  0.522981 | fidelity:  0.987791 | Sz: -0.000000 | S^2:  0.031051
iter: 19 | loss: -2.780972 | norm:  0.393027 | fidelity:  0.988664 | Sz:  0.000000 | S^2:  0.042688
iter: 20 | loss: -2.780619 | norm:  0.334300 | fidelity:  0.987609 | Sz: -0.000000 | S^2:  0.053839
iter: 21 | loss: -2.776695 | norm:  0.379229 | fidelity:  0.985337 | Sz: -0.000000 | S^2:  0.062936
iter: 22 | loss: -2.771325 | norm:  0.454696 | fidelity:  0.982671 | Sz:  0.000000 | S^2:  0.068789
iter: 23 | loss: -2.766850 | norm:  0.498446 | fidelity:  0.980310 | Sz:  0.000000 | S^2:  0.070955
iter: 24 | loss: -2.764593 | norm:  0.492433 | fidelity:  0.978671 | Sz:  0.000000 | S^2:  0.069764
iter: 25 | loss: -2.764463 | norm:  0.446009 | fidelity:  0.977863 | Sz:  0.000000 | S^2:  0.066076
iter: 26 | loss: -2.765446 | norm:  0.386702 | fidelity:  0.977780 | Sz: -0.000000 | S^2:  0.060943
iter: 27 | loss: -2.766530 | norm:  0.350420 | fidelity:  0.978257 | Sz:  0.000000 | S^2:  0.055323
iter: 28 | loss: -2.767317 | norm:  0.354756 | fidelity:  0.979160 | Sz:  0.000000 | S^2:  0.049892
iter: 29 | loss: -2.768224 | norm:  0.379157 | fidelity:  0.980447 | Sz:  0.000000 | S^2:  0.045011
iter: 30 | loss: -2.769919 | norm:  0.392426 | fidelity:  0.982081 | Sz:  0.000000 | S^2:  0.040805
iter: 31 | loss: -2.772780 | norm:  0.378980 | fidelity:  0.983976 | Sz:  0.000000 | S^2:  0.037312
iter: 32 | loss: -2.776583 | norm:  0.340488 | fidelity:  0.985954 | Sz:  0.000000 | S^2:  0.034583
iter: 33 | loss: -2.780663 | norm:  0.290089 | fidelity:  0.987766 | Sz:  0.000000 | S^2:  0.032691
iter: 34 | loss: -2.784334 | norm:  0.245011 | fidelity:  0.989180 | Sz: -0.000000 | S^2:  0.031660
iter: 35 | loss: -2.787182 | norm:  0.216529 | fidelity:  0.990034 | Sz: -0.000000 | S^2:  0.031383
iter: 36 | loss: -2.789142 | norm:  0.202945 | fidelity:  0.990273 | Sz:  0.000000 | S^2:  0.031591
iter: 37 | loss: -2.790367 | norm:  0.196818 | fidelity:  0.989944 | Sz:  0.000000 | S^2:  0.031904
iter: 38 | loss: -2.791051 | norm:  0.195799 | fidelity:  0.989151 | Sz:  0.000000 | S^2:  0.031944
iter: 39 | loss: -2.791429 | norm:  0.200205 | fidelity:  0.988031 | Sz: -0.000000 | S^2:  0.031461
iter: 40 | loss: -2.791741 | norm:  0.204399 | fidelity:  0.986722 | Sz: -0.000000 | S^2:  0.030400
iter: 41 | loss: -2.792173 | norm:  0.197203 | fidelity:  0.985335 | Sz:  0.000000 | S^2:  0.028895
iter: 42 | loss: -2.792707 | norm:  0.170329 | fidelity:  0.983932 | Sz:  0.000000 | S^2:  0.027189
iter: 43 | loss: -2.793118 | norm:  0.126200 | fidelity:  0.982542 | Sz:  0.000000 | S^2:  0.025532
iter: 44 | loss: -2.793096 | norm:  0.088266 | fidelity:  0.981185 | Sz: -0.000000 | S^2:  0.024112
iter: 45 | loss: -2.792524 | norm:  0.101346 | fidelity:  0.979911 | Sz: -0.000000 | S^2:  0.023021
iter: 46 | loss: -2.791623 | norm:  0.145236 | fidelity:  0.978813 | Sz:  0.000000 | S^2:  0.022269
iter: 47 | loss: -2.790837 | norm:  0.177740 | fidelity:  0.978000 | Sz: -0.000000 | S^2:  0.021821
iter: 48 | loss: -2.790541 | norm:  0.184373 | fidelity:  0.977550 | Sz:  0.000000 | S^2:  0.021637
iter: 49 | loss: -2.790771 | norm:  0.165900 | fidelity:  0.977463 | Sz:  0.000000 | S^2:  0.021687
iter: 50 | loss: -2.791303 | norm:  0.133713 | fidelity:  0.977689 | Sz:  0.000000 | S^2:  0.021946
iter: 51 | loss: -2.791838 | norm:  0.106978 | fidelity:  0.978153 | Sz:  0.000000 | S^2:  0.022379
iter: 52 | loss: -2.792202 | norm:  0.101066 | fidelity:  0.978786 | Sz: -0.000000 | S^2:  0.022922
iter: 53 | loss: -2.792433 | norm:  0.108127 | fidelity:  0.979557 | Sz:  0.000000 | S^2:  0.023489
iter: 54 | loss: -2.792648 | norm:  0.111867 | fidelity:  0.980449 | Sz: -0.000000 | S^2:  0.023986
iter: 55 | loss: -2.792921 | norm:  0.107042 | fidelity:  0.981440 | Sz: -0.000000 | S^2:  0.024348
iter: 56 | loss: -2.793249 | norm:  0.096688 | fidelity:  0.982494 | Sz:  0.000000 | S^2:  0.024558
iter: 57 | loss: -2.793579 | norm:  0.084949 | fidelity:  0.983558 | Sz: -0.000000 | S^2:  0.024660
iter: 58 | loss: -2.793877 | norm:  0.072769 | fidelity:  0.984581 | Sz:  0.000000 | S^2:  0.024741
iter: 59 | loss: -2.794100 | norm:  0.059757 | fidelity:  0.985511 | Sz: -0.000000 | S^2:  0.024907
iter: 60 | loss: -2.794221 | norm:  0.049755 | fidelity:  0.986310 | Sz: -0.000000 | S^2:  0.025243
iter: 61 | loss: -2.794229 | norm:  0.050194 | fidelity:  0.986955 | Sz:  0.000000 | S^2:  0.025792
iter: 62 | loss: -2.794171 | norm:  0.058457 | fidelity:  0.987446 | Sz: -0.000000 | S^2:  0.026546
iter: 63 | loss: -2.794097 | norm:  0.063822 | fidelity:  0.987797 | Sz:  0.000000 | S^2:  0.027454
iter: 64 | loss: -2.794054 | norm:  0.060529 | fidelity:  0.988031 | Sz:  0.000000 | S^2:  0.028434
iter: 65 | loss: -2.794029 | norm:  0.050574 | fidelity:  0.988169 | Sz:  0.000000 | S^2:  0.029386
iter: 66 | loss: -2.793996 | norm:  0.043791 | fidelity:  0.988236 | Sz:  0.000000 | S^2:  0.030205
iter: 67 | loss: -2.793942 | norm:  0.048879 | fidelity:  0.988258 | Sz: -0.000000 | S^2:  0.030795
iter: 68 | loss: -2.793892 | norm:  0.058187 | fidelity:  0.988261 | Sz:  0.000000 | S^2:  0.031084
iter: 69 | loss: -2.793887 | norm:  0.061345 | fidelity:  0.988266 | Sz:  0.000000 | S^2:  0.031041
iter: 70 | loss: -2.793943 | norm:  0.055295 | fidelity:  0.988276 | Sz: -0.000000 | S^2:  0.030685
iter: 71 | loss: -2.794026 | norm:  0.043429 | fidelity:  0.988281 | Sz:  0.000000 | S^2:  0.030083
iter: 72 | loss: -2.794098 | norm:  0.034302 | fidelity:  0.988260 | Sz: -0.000000 | S^2:  0.029338
iter: 73 | loss: -2.794143 | norm:  0.034850 | fidelity:  0.988194 | Sz: -0.000000 | S^2:  0.028563
iter: 74 | loss: -2.794163 | norm:  0.039189 | fidelity:  0.988069 | Sz:  0.000000 | S^2:  0.027862
iter: 75 | loss: -2.794201 | norm:  0.039818 | fidelity:  0.987888 | Sz:  0.000000 | S^2:  0.027312
iter: 76 | loss: -2.794249 | norm:  0.035518 | fidelity:  0.987650 | Sz:  0.000000 | S^2:  0.026954
iter: 77 | loss: -2.794297 | norm:  0.028711 | fidelity:  0.987365 | Sz:  0.000000 | S^2:  0.026790
iter: 78 | loss: -2.794338 | norm:  0.022306 | fidelity:  0.987052 | Sz:  0.000000 | S^2:  0.026793
iter: 79 | loss: -2.794356 | norm:  0.017704 | fidelity:  0.986728 | Sz:  0.000000 | S^2:  0.026910
iter: 80 | loss: -2.794355 | norm:  0.015981 | fidelity:  0.986418 | Sz:  0.000000 | S^2:  0.027073
iter: 81 | loss: -2.794344 | norm:  0.018692 | fidelity:  0.986142 | Sz: -0.000000 | S^2:  0.027218
iter: 82 | loss: -2.794317 | norm:  0.023603 | fidelity:  0.985913 | Sz:  0.000000 | S^2:  0.027290
iter: 83 | loss: -2.794303 | norm:  0.026555 | fidelity:  0.985746 | Sz: -0.000000 | S^2:  0.027261
iter: 84 | loss: -2.794298 | norm:  0.025244 | fidelity:  0.985639 | Sz: -0.000000 | S^2:  0.027128
iter: 85 | loss: -2.794306 | norm:  0.020270 | fidelity:  0.985590 | Sz: -0.000000 | S^2:  0.026913
iter: 86 | loss: -2.794309 | norm:  0.015820 | fidelity:  0.985585 | Sz: -0.000000 | S^2:  0.026658
iter: 87 | loss: -2.794306 | norm:  0.017048 | fidelity:  0.985617 | Sz:  0.000000 | S^2:  0.026413
iter: 88 | loss: -2.794304 | norm:  0.020836 | fidelity:  0.985677 | Sz:  0.000000 | S^2:  0.026222
iter: 89 | loss: -2.794306 | norm:  0.022087 | fidelity:  0.985756 | Sz:  0.000000 | S^2:  0.026118
iter: 90 | loss: -2.794313 | norm:  0.019729 | fidelity:  0.985847 | Sz:  0.000000 | S^2:  0.026117
iter: 91 | loss: -2.794329 | norm:  0.015576 | fidelity:  0.985947 | Sz:  0.000000 | S^2:  0.026216
iter: 92 | loss: -2.794342 | norm:  0.012774 | fidelity:  0.986050 | Sz:  0.000000 | S^2:  0.026395
iter: 93 | loss: -2.794349 | norm:  0.012425 | fidelity:  0.986153 | Sz: -0.000000 | S^2:  0.026622
iter: 94 | loss: -2.794354 | norm:  0.012269 | fidelity:  0.986259 | Sz: -0.000000 | S^2:  0.026862
iter: 95 | loss: -2.794359 | norm:  0.011034 | fidelity:  0.986368 | Sz:  0.000000 | S^2:  0.027084
iter: 96 | loss: -2.794362 | norm:  0.009634 | fidelity:  0.986477 | Sz: -0.000000 | S^2:  0.027264

learning rate =  0.010276610404253006
Find operators
-1j [5^ 0^ 6 3] + 1j [6^ 3^ 5 0]
1j [4^ 1^ 7 2] + -1j [7^ 2^ 4 1]
1j [4^ 1^ 6 3] + -1j [6^ 3^ 4 1]
1j [5^ 0^ 7 2] + -1j [7^ 2^ 5 0]
with max gradients
[0.28749666, 0.28749663, 0.043519054, 0.04208814]

iter: 97 | loss: -2.794367 | norm:  0.411166 | fidelity:  0.986586 | Sz:  0.000000 | S^2:  0.027390
iter: 98 | loss: -2.798250 | norm:  0.422390 | fidelity:  0.986895 | Sz:  0.000000 | S^2:  0.026343
iter: 99 | loss: -2.805049 | norm:  0.332473 | fidelity:  0.988328 | Sz: -0.000000 | S^2:  0.025249
iter: 100 | loss: -2.809802 | norm:  0.303362 | fidelity:  0.990338 | Sz: -0.000000 | S^2:  0.017499
iter: 101 | loss: -2.813918 | norm:  0.291247 | fidelity:  0.992655 | Sz:  0.000000 | S^2:  0.010228
iter: 102 | loss: -2.817901 | norm:  0.259103 | fidelity:  0.994826 | Sz: -0.000000 | S^2:  0.005315
iter: 103 | loss: -2.821396 | norm:  0.216040 | fidelity:  0.996608 | Sz:  0.000000 | S^2:  0.002689
iter: 104 | loss: -2.824264 | norm:  0.167711 | fidelity:  0.997987 | Sz:  0.000000 | S^2:  0.001493
iter: 105 | loss: -2.826273 | norm:  0.128550 | fidelity:  0.998962 | Sz: -0.000000 | S^2:  0.000930
iter: 106 | loss: -2.827366 | norm:  0.113937 | fidelity:  0.999549 | Sz:  0.000000 | S^2:  0.000590
iter: 107 | loss: -2.827798 | norm:  0.114620 | fidelity:  0.999831 | Sz:  0.000000 | S^2:  0.000316
iter: 108 | loss: -2.827839 | norm:  0.111344 | fidelity:  0.999890 | Sz: -0.000000 | S^2:  0.000093
iter: 109 | loss: -2.827682 | norm:  0.100442 | fidelity:  0.999796 | Sz:  0.000000 | S^2:  0.000048
iter: 110 | loss: -2.827280 | norm:  0.099618 | fidelity:  0.999560 | Sz:  0.000000 | S^2:  0.000377
iter: 111 | loss: -2.826660 | norm:  0.119734 | fidelity:  0.999229 | Sz: -0.000000 | S^2:  0.001173
iter: 112 | loss: -2.825960 | norm:  0.143879 | fidelity:  0.998877 | Sz: -0.000000 | S^2:  0.002301
iter: 113 | loss: -2.825438 | norm:  0.154700 | fidelity:  0.998610 | Sz: -0.000000 | S^2:  0.003400
iter: 114 | loss: -2.825220 | norm:  0.149887 | fidelity:  0.998494 | Sz:  0.000000 | S^2:  0.004078
iter: 115 | loss: -2.825270 | norm:  0.138777 | fidelity:  0.998541 | Sz:  0.000000 | S^2:  0.004120
iter: 116 | loss: -2.825499 | norm:  0.132182 | fidelity:  0.998714 | Sz:  0.000000 | S^2:  0.003587
iter: 117 | loss: -2.825827 | norm:  0.132082 | fidelity:  0.998955 | Sz: -0.000000 | S^2:  0.002753
iter: 118 | loss: -2.826214 | norm:  0.131095 | fidelity:  0.999206 | Sz: -0.000000 | S^2:  0.001917
iter: 119 | loss: -2.826649 | norm:  0.122394 | fidelity:  0.999434 | Sz: -0.000000 | S^2:  0.001259
iter: 120 | loss: -2.827097 | norm:  0.105549 | fidelity:  0.999623 | Sz: -0.000000 | S^2:  0.000816
iter: 121 | loss: -2.827500 | norm:  0.084848 | fidelity:  0.999762 | Sz:  0.000000 | S^2:  0.000540
iter: 122 | loss: -2.827830 | norm:  0.065633 | fidelity:  0.999857 | Sz:  0.000000 | S^2:  0.000360
iter: 123 | loss: -2.828071 | norm:  0.052198 | fidelity:  0.999917 | Sz:  0.000000 | S^2:  0.000228
iter: 124 | loss: -2.828202 | norm:  0.045923 | fidelity:  0.999945 | Sz:  0.000000 | S^2:  0.000123
iter: 125 | loss: -2.828246 | norm:  0.043310 | fidelity:  0.999950 | Sz:  0.000000 | S^2:  0.000043
iter: 126 | loss: -2.828239 | norm:  0.040009 | fidelity:  0.999941 | Sz:  0.000000 | S^2:  0.000003
iter: 127 | loss: -2.828206 | norm:  0.037487 | fidelity:  0.999924 | Sz: -0.000000 | S^2:  0.000016
iter: 128 | loss: -2.828144 | norm:  0.041753 | fidelity:  0.999903 | Sz:  0.000000 | S^2:  0.000094
iter: 129 | loss: -2.828069 | norm:  0.051767 | fidelity:  0.999882 | Sz: -0.000000 | S^2:  0.000234
iter: 130 | loss: -2.827978 | norm:  0.060029 | fidelity:  0.999856 | Sz:  0.000000 | S^2:  0.000412
iter: 131 | loss: -2.827926 | norm:  0.061946 | fidelity:  0.999841 | Sz: -0.000000 | S^2:  0.000578
iter: 132 | loss: -2.827917 | norm:  0.058971 | fidelity:  0.999839 | Sz:  0.000000 | S^2:  0.000677
iter: 133 | loss: -2.827926 | norm:  0.057048 | fidelity:  0.999845 | Sz: -0.000000 | S^2:  0.000689
iter: 134 | loss: -2.827962 | norm:  0.058778 | fidelity:  0.999863 | Sz: -0.000000 | S^2:  0.000624
iter: 135 | loss: -2.828010 | norm:  0.059369 | fidelity:  0.999883 | Sz:  0.000000 | S^2:  0.000510
iter: 136 | loss: -2.828089 | norm:  0.054313 | fidelity:  0.999909 | Sz:  0.000000 | S^2:  0.000375
iter: 137 | loss: -2.828184 | norm:  0.044409 | fidelity:  0.999935 | Sz:  0.000000 | S^2:  0.000246
iter: 138 | loss: -2.828270 | norm:  0.034496 | fidelity:  0.999958 | Sz:  0.000000 | S^2:  0.000140
iter: 139 | loss: -2.828324 | norm:  0.028813 | fidelity:  0.999973 | Sz:  0.000000 | S^2:  0.000068
iter: 140 | loss: -2.828366 | norm:  0.025588 | fidelity:  0.999986 | Sz: -0.000000 | S^2:  0.000026
iter: 141 | loss: -2.828399 | norm:  0.020952 | fidelity:  0.999995 | Sz:  0.000000 | S^2:  0.000006
iter: 142 | loss: -2.828413 | norm:  0.015614 | fidelity:  0.999999 | Sz:  0.000000 | S^2:  0.000000
iter: 143 | loss: -2.828397 | norm:  0.014356 | fidelity:  0.999993 | Sz:  0.000000 | S^2:  0.000005
iter: 144 | loss: -2.828383 | norm:  0.018379 | fidelity:  0.999989 | Sz:  0.000000 | S^2:  0.000022
iter: 145 | loss: -2.828359 | norm:  0.023655 | fidelity:  0.999983 | Sz: -0.000000 | S^2:  0.000048
iter: 146 | loss: -2.828334 | norm:  0.027647 | fidelity:  0.999976 | Sz:  0.000000 | S^2:  0.000078
iter: 147 | loss: -2.828323 | norm:  0.029371 | fidelity:  0.999973 | Sz:  0.000000 | S^2:  0.000104
iter: 148 | loss: -2.828321 | norm:  0.028986 | fidelity:  0.999972 | Sz:  0.000000 | S^2:  0.000121
iter: 149 | loss: -2.828317 | norm:  0.027744 | fidelity:  0.999970 | Sz:  0.000000 | S^2:  0.000127
iter: 150 | loss: -2.828332 | norm:  0.026820 | fidelity:  0.999974 | Sz: -0.000000 | S^2:  0.000121
iter: 151 | loss: -2.828341 | norm:  0.025457 | fidelity:  0.999976 | Sz:  0.000000 | S^2:  0.000105
iter: 152 | loss: -2.828363 | norm:  0.022135 | fidelity:  0.999981 | Sz:  0.000000 | S^2:  0.000079
iter: 153 | loss: -2.828385 | norm:  0.017222 | fidelity:  0.999987 | Sz:  0.000000 | S^2:  0.000050
iter: 154 | loss: -2.828398 | norm:  0.013070 | fidelity:  0.999990 | Sz: -0.000000 | S^2:  0.000026
iter: 155 | loss: -2.828412 | norm:  0.011373 | fidelity:  0.999994 | Sz: -0.000000 | S^2:  0.000009
iter: 156 | loss: -2.828420 | norm:  0.010603 | fidelity:  0.999997 | Sz: -0.000000 | S^2:  0.000001
iter: 157 | loss: -2.828419 | norm:  0.008864 | fidelity:  0.999996 | Sz: -0.000000 | S^2:  0.000001


convergence criterion has satisfied, break the loop!
total run time:  1888.1472356319427
